# -*- coding: utf-8 -*-
"""Digit Recognition .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oz_z165lHXbh6GFVdTttomI_pGGCE9gt

#Imports
"""

import torchvision.transforms as transforms
from torchvision.datasets import MNIST
from torch.utils.data import DataLoader
from torch import nn
import torch
import torch.optim as optim
from tqdm import tqdm

"""#Data"""

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

trainSet = MNIST(root='./data', train=True, download=True, transform=transform)

testSet = MNIST(root='./data', train=False, download=True, transform=transform)

trainLoader = DataLoader(trainSet, batch_size=64, shuffle=True)
testLoader = DataLoader(testSet, batch_size=64, shuffle=False)

"""#Models

##Sub Layers
"""

class MainLayer(nn.Module):
  def __init__(self, inputSize, outputSize):
    super().__init__()
    self.sequential = nn.Sequential(
      nn.Linear(inputSize, outputSize),
      nn.ReLU(),
    )

class InputLayer(MainLayer):
  def forward(self, input):
    return self.sequential(input)

class HiddenLayer(MainLayer):
  def forward(self, input):
    return self.sequential(input)

class OutputLayer(MainLayer):
  def forward(self, input):
    return self.sequential(input)

"""##Recognizer Model"""

class RecognizerModel(nn.Module):
  def __init__(self, inputSize, hiddenSize, outputSize):
    super().__init__()
    self.sequential = nn.Sequential(
      InputLayer(inputSize, hiddenSize),

      HiddenLayer(hiddenSize, hiddenSize),
      HiddenLayer(hiddenSize, hiddenSize),

      OutputLayer(hiddenSize, outputSize)
    )

  def forward(self, input):
    flattenImage = input.flatten(start_dim=1)
    return self.sequential(flattenImage)

recognizerModel=RecognizerModel(784, 256, 10)

"""#Optimizer & Loss"""

loss = nn.CrossEntropyLoss()
optimizer = optim.SGD(recognizerModel.parameters(), lr=0.01)

def parametersCount(model):
  return sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"model parameters: {parametersCount(recognizerModel):,}")

"""#Training"""

class Train():
    def __init__(self, model: nn.Module, loss, optimizer, epochs: int, trainLoader: DataLoader, testLoader: DataLoader):
        self.model = model
        self.loss = loss
        self.optimizer = optimizer
        self.epochs = epochs
        self.trainLoader = trainLoader
        self.testLoader = testLoader
        self.startTraining()

    def startTraining(self):
        for epoch in range(self.epochs):
          trainLoss = self.train(epoch)
          self.test()

          print(f"Epoch [{epoch + 1}/{self.epochs}], Training Loss: {trainLoss.item()}")


    def train(self, epoch):
      self.model.train()
      for input, target in self.trainLoader:
        self.optimizer.zero_grad()

        output = self.model(input)
        loss = self.loss(output, target)
        loss.backward()

        self.optimizer.step()

      return loss

    def test(self):
      self.model.eval()
      with torch.no_grad():
        total_correct = 0
        total_samples = 0
        for input, target in self.testLoader:
          output = self.model(input)
          _, predicted = torch.max(output, 1)
          total_correct += (predicted == target).sum().item()
          total_samples += target.size(0)

        accuracy = total_correct / total_samples * 100
        print(f"Test Accuracy: {accuracy:.2f}%")
Train(recognizerModel, loss, optimizer, 10, trainLoader, testLoader)